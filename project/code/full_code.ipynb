{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428ad085-a0c2-44a5-869a-1d076e44dd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import logging\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from transformers import logging, pipeline\n",
    "from summarizer import Summarizer\n",
    "import os\n",
    "import warnings\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7063b67b-130f-44cc-b8c1-dbab7e4b9e8d",
   "metadata": {},
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a86d0e-e55b-43d6-a051-d168c52a4206",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('raw_partner_headlines.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd8695e-8f30-4a9a-ba23-f9f05b5f7b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6edb68-cc6d-4c71-b3ff-2b04bd1ecc0e",
   "metadata": {},
   "source": [
    "# Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe3db7a-5bc8-44bd-872b-be23a27e5fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30852fe-7087-4072-be7f-b3e2436780a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publishers\n",
    "df.groupby('publisher').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e6c058-bdd5-4cac-a00b-986f81637d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Companies\n",
    "df.groupby('stock').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11af3c31-28b5-4608-89f3-c985bb9477ac",
   "metadata": {},
   "source": [
    "# Filtering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45377bc8-fe99-4627-9e44-d10778961b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading S&P companies\n",
    "def get_sp500_symbols():\n",
    "    table = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "    sp500 = table[0]\n",
    "    symbols = sp500['Symbol'].tolist()\n",
    "    return symbols\n",
    "\n",
    "sp500_symbols = get_sp500_symbols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60870e2-9b97-48d8-879f-de61a47b788f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering Data\n",
    "filtered_df = df[df['stock'].isin(sp500_symbols)]\n",
    "filtered_df_10 = filtered_df.groupby('stock').size().sort_values(ascending=False)[:10]\n",
    "print(filtered_df_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6a4abe-ca34-45f2-82aa-8c2148bd2f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of filtered news\n",
    "filtered_df_10.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1294f935-4e5c-4647-9835-bbdd9ed61a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_symbols_list = filtered_df_10.index.tolist()\n",
    "print(stock_symbols_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f8f876-e752-431f-98a0-d114b9e971b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[df['stock'].isin(stock_symbols_list)]\n",
    "filtered_df = filtered_df.reset_index(drop=True)\n",
    "filtered_df = filtered_df.drop(['Unnamed: 0', 'url', 'publisher'], axis=1)\n",
    "filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n",
    "filtered_df['date'] = filtered_df['date'].dt.date\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e26051-18f3-4afa-b0a5-85134b340d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date = filtered_df['date'].max().strftime('%Y-%m-%d')\n",
    "start_date = filtered_df['date'].min().strftime('%Y-%m-%d')\n",
    "print(end_date)\n",
    "print(start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb300bf3-8258-4d38-b2a8-215a43aefe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting data for S&P\n",
    "spy = yf.Ticker(\"SPY\")\n",
    "hist_spy = spy.history(start=start_date, end=end_date)\n",
    "\n",
    "# Calculating daily change\n",
    "hist_spy['Daily Change'] = round(((hist_spy['Close'] - hist_spy['Open'])/hist_spy['Open']) * 100, 2)\n",
    "hist_spy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21515ee-8c9e-4de2-82b8-0d549fb8b29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store DataFrame for each symbol\n",
    "dfs = []\n",
    "\n",
    "for symbol in stock_symbols_list:\n",
    "    ticker = yf.Ticker(symbol)\n",
    "    hist = ticker.history(start=start_date, end=end_date)\n",
    "    hist['Daily Change'] = round(((hist['Close'] - hist['Open'])/hist['Open']) * 100, 2)\n",
    "    daily_change_diff = hist['Daily Change'] - hist_spy['Daily Change']\n",
    "    df2 = pd.DataFrame(daily_change_diff)\n",
    "    df2.reset_index(inplace=True)\n",
    "    df2['Date'] = pd.to_datetime(df2['Date'])\n",
    "    df2['Date'] = df2['Date'].dt.date\n",
    "    \n",
    "    df_filtered = filtered_df[filtered_df['stock'] == symbol]\n",
    "    merged_df = pd.merge(df_filtered, df2, left_on='date', right_on='Date', how='left')\n",
    "    merged_df.drop('Date', axis=1, inplace=True)\n",
    "    dfs.append(merged_df)\n",
    "\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "final_df = final_df.dropna(subset=['Daily Change'])\n",
    "final_df = final_df.reset_index(drop=True)\n",
    "\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bf2d19-2574-4881-8d61-e1c32dcf2c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_change(value, threshold):\n",
    "    if value > threshold:\n",
    "        return 'positive'\n",
    "    elif value <= -threshold:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1d5a9d-5392-472b-a73e-4b9aa54d0879",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.4\n",
    "final_df['Change Category'] = final_df['Daily Change'].apply(lambda x: categorize_change(x, threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eb9e66-d409-460a-878f-ee521cd1e083",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f8f625-f4ed-422c-a151-6b77d9260d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = final_df.groupby('Change Category').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c0d5e6-550c-4d2a-90f3-152723618c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts.plot(kind='bar', color=['red', 'blue', 'green'])\n",
    "\n",
    "plt.title('Counts by Change Category')\n",
    "plt.xlabel('Change Category')\n",
    "plt.ylabel('Counts')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800e2dca-a78d-4192-a988-27cf073c80d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09680586-84bf-4a3a-bbc2-4360e620a116",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0800a6e-91cb-470c-a528-9a43f482fb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd04fe9-b281-4e05-9791-1975addefc07",
   "metadata": {},
   "source": [
    "## Prepating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876d5f6c-2955-46a5-a040-5b3673b9a1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to match expected input ('text' and 'labels')\n",
    "df['text'] = df['headline']  # Copying headline to a new column named 'text'\n",
    "df['label'] = df['Change Category']  # Renaming 'Change Category' for clarity\n",
    "\n",
    "# Label encoding for 'Change Category'\n",
    "le = LabelEncoder()\n",
    "df['labels'] = le.fit_transform(df['label'])\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_df, test_df = train_test_split(df[['text', 'labels']], test_size=0.2, random_state=42)\n",
    "\n",
    "# Keep only the necessary columns in the train and test sets\n",
    "train_df = train_df[['text', 'labels']]\n",
    "test_df = test_df[['text', 'labels']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff05a82-aad0-4710-901e-b1eaa1220031",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6361cdfb-7e5c-42fc-a801-a275ec86234d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model arguments\n",
    "model_args = ClassificationArgs(\n",
    "    num_train_epochs=5,\n",
    "    use_multiprocessing=False,\n",
    "    use_multiprocessing_for_evaluation=False,\n",
    "    output_dir='output',\n",
    "    overwrite_output_dir=True\n",
    ")\n",
    "\n",
    "# Initialize a ClassificationModel using RoBERTa\n",
    "model = ClassificationModel(\n",
    "    \"bert\",  # Specify the model type as RoBERTa\n",
    "    \"bert-base-uncased\",  # Use a RoBERTa base model. Adjust the model checkpoint as needed.\n",
    "    num_labels=len(le.classes_),\n",
    "    args=model_args,\n",
    "    use_cuda=True\n",
    ")\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "# Train the model on the training dataset\n",
    "model.train_model(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f60baa-b670-4e77-bef3-1827862ab78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test dataset\n",
    "predictions, raw_outputs = model.predict(test_df['text'].tolist())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_df['labels'], predictions)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e87b88-177a-45b1-9649-6dacb6825f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example purposes, let's say we have the following:\n",
    "actual_labels = test_df['labels']\n",
    "predicted_labels = predictions  # From your model's predictions on the test set\n",
    "\n",
    "# Calculate precision, recall, F1-score, and support for each class\n",
    "report = classification_report(actual_labels, predicted_labels, target_names=le.classes_)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Calculate and display the confusion matrix\n",
    "cm = confusion_matrix(actual_labels, predicted_labels)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "cm_df = pd.DataFrame(cm, index=le.classes_, columns=le.classes_)\n",
    "print(\"Confusion Matrix with Labels:\\n\", cm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ddc260-b6f4-4682-8d80-0db4bcdb2ed5",
   "metadata": {},
   "source": [
    "## Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d215f42-1ae4-424f-a5fb-a80f1c92faf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model arguments\n",
    "model_args = ClassificationArgs(\n",
    "    num_train_epochs=5,\n",
    "    use_multiprocessing=False,\n",
    "    use_multiprocessing_for_evaluation=False,\n",
    "    output_dir='output',\n",
    "    overwrite_output_dir=True\n",
    ")\n",
    "\n",
    "# Initialize a ClassificationModel using RoBERTa\n",
    "model = ClassificationModel(\n",
    "    \"roberta\",  # Specify the model type as RoBERTa\n",
    "    \"roberta-base\",  # Use a RoBERTa base model. Adjust the model checkpoint as needed.\n",
    "    num_labels=len(le.classes_),\n",
    "    args=model_args,\n",
    "    use_cuda=True\n",
    ")\n",
    "\n",
    "# Train the model on the training dataset\n",
    "model.train_model(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af98929-6e11-4e20-9b25-06e3a71ed99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test dataset\n",
    "predictions, raw_outputs = model.predict(test_df['text'].tolist())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_df['labels'], predictions)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5bc706-7533-4a72-b25f-df88e9226f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example purposes, let's say we have the following:\n",
    "actual_labels = test_df['labels']\n",
    "predicted_labels = predictions  # From your model's predictions on the test set\n",
    "\n",
    "# Calculate precision, recall, F1-score, and support for each class\n",
    "report = classification_report(actual_labels, predicted_labels, target_names=le.classes_)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Calculate and display the confusion matrix\n",
    "cm = confusion_matrix(actual_labels, predicted_labels)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "cm_df = pd.DataFrame(cm, index=le.classes_, columns=le.classes_)\n",
    "print(\"Confusion Matrix with Labels:\\n\", cm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f485d3-db23-4aa5-b788-85aff5234e61",
   "metadata": {},
   "source": [
    "## Electra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa73e11-c6a8-40d1-b5b4-2f9ca16cec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model arguments\n",
    "model_args = ClassificationArgs(\n",
    "    num_train_epochs=5,\n",
    "    use_multiprocessing=False,\n",
    "    use_multiprocessing_for_evaluation=False,\n",
    "    output_dir='output',\n",
    "    overwrite_output_dir=True\n",
    ")\n",
    "\n",
    "# Initialize a ClassificationModel using RoBERTa\n",
    "model = ClassificationModel(\n",
    "    \"electra\",\n",
    "    \"google/electra-small-discriminator\",\n",
    "    num_labels=len(le.classes_),\n",
    "    args=model_args,\n",
    "    use_cuda=True\n",
    ")\n",
    "\n",
    "# Train the model on the training dataset\n",
    "model.train_model(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2a1daa-4801-4e85-a543-8c09c98d0ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test dataset\n",
    "predictions, raw_outputs = model.predict(test_df['text'].tolist())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_df['labels'], predictions)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f52c26-bec9-4511-bb08-4bf2aa119826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example purposes, let's say we have the following:\n",
    "actual_labels = test_df['labels']\n",
    "predicted_labels = predictions  # From your model's predictions on the test set\n",
    "\n",
    "# Calculate precision, recall, F1-score, and support for each class\n",
    "report = classification_report(actual_labels, predicted_labels, target_names=le.classes_)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Calculate and display the confusion matrix\n",
    "cm = confusion_matrix(actual_labels, predicted_labels)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Optionally, to make it easier to read, you can print the confusion matrix with labels\n",
    "cm_df = pd.DataFrame(cm, index=le.classes_, columns=le.classes_)\n",
    "print(\"Confusion Matrix with Labels:\\n\", cm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb82d60-a148-407c-bb90-173598224b83",
   "metadata": {},
   "source": [
    "# Flan-t5-large Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e910ce-aa1d-42b2-92fa-f65d3c139799",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\", max_new_tokens = 500)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ed759f-2153-4ad8-a8b6-272830f28cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(instance, examples, zero_shot = False):\n",
    "    prompt = 'In this task you have to determine if a piece of text might lead to a positive or negative change in the price of a stock mentioned in the headline. If you are unsure whether a headline would lead to a change in the stock price, err on the side of caution and label it as neutral.'\n",
    "    \n",
    "    prompt_end = 'With these instructions in mind and a piece of text, please reply with either of the three options and nothing else: 1) positive, 2) negative, 3) neutral. How this headline might be labelled based on the past guidelines: '\n",
    "    \n",
    "    if zero_shot: # for zero-shot say zero_shot = True\n",
    "        return f'''{prompt} \\n \\n {prompt_end} {instance} ?'''\n",
    "    \n",
    "    else: # for few-shot say zero_shot = False\n",
    "        examples_str = ' '.join([f'Example for \"{label}\": {example}' for label, example in examples.items()])\n",
    "\n",
    "    return f'''{prompt} {examples_str} \\n \\n {prompt_end} \"{instance}\" ?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4c55c3-e28c-4f53-b3b3-cda141247b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_examples = {'potitive': 'SpartanNash (SPTN) Q1 Earnings Surpass Estimates, Sales Up',\n",
    "           'negative': 'Abercrombie (ANF) Q1 Loss Wider Than Expected, Sales Fall Y/Y',\n",
    "           'neutral': 'Kroger Achieves New Zero Hunger | Zero Waste Milestones'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204b15bd-1bed-45cb-8684-c5a1cb95001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_labelling_headlines(data, use_few_shot):\n",
    "    all_responses = []\n",
    "\n",
    "    for _, row in tqdm(data.iterrows(), total=len(data)):\n",
    "        instance = row['headline'][:512] \n",
    "        if use_few_shot:\n",
    "            prompt = make_prompt(instance, examples=headlines_examples, zero_shot=False)\n",
    "        else:\n",
    "            prompt = make_prompt(instance, examples=headlines_examples, zero_shot=True)\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "        outputs = model.generate(**inputs)\n",
    "        answer = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "        all_responses.append(answer)\n",
    "        \n",
    "    return all_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c316b9-5163-4cfa-9231-3ffd61188680",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_responses_zero_shot = model_labelling_headlines(final_df, use_few_shot=False)\n",
    "all_responses_few_shot = model_labelling_headlines(final_df, use_few_shot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3899e50d-8390-44d5-a24d-32ff8574d1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['headline', 'date', 'label', 'zero_shot', 'few_shot'])\n",
    "results['headline'] = final_df['headline']\n",
    "results['date'] = final_df['date']\n",
    "results['label'] = final_df['Change Category']\n",
    "results['zero_shot'] = all_responses_zero_shot\n",
    "results['few_shot'] = all_responses_few_shot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1da3ce-4a5d-4a2d-a2fd-1ba68b6356d6",
   "metadata": {},
   "source": [
    "## Evaluation of the results Flan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec64323f-b9db-4afa-9395-bc2a6fff06e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "different_values = (results['zero_shot'] != results['few_shot']).sum()\n",
    "print(\"Number of different values between 'zero_shot' and 'few_shot':\", different_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54893224-faba-4400-bb89-a6b9266b44af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column for comparing true label with zero_shot and few_shot predictions\n",
    "results['zero_shot_correct'] = (results['label'] == results['zero_shot'])\n",
    "results['few_shot_correct'] = (results['label'] == results['few_shot'])\n",
    "\n",
    "# Calculate the accuracy for zero_shot and few_shot\n",
    "zero_shot_accuracy = results['zero_shot_correct'].mean() * 100\n",
    "few_shot_accuracy = results['few_shot_correct'].mean() * 100\n",
    "\n",
    "# Print the accuracies\n",
    "print(\"Zero Shot Accuracy: {:.2f}%\".format(zero_shot_accuracy))\n",
    "print(\"Few Shot Accuracy: {:.2f}%\".format(few_shot_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b75e83-5673-4ebe-a21d-ab0bcc2e567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_macro_f1(df, column_name):\n",
    "    y_true = df['label']\n",
    "    y_pred = df[column_name]\n",
    "    f1 = round(f1_score(y_true, y_pred, average='macro'), 3)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9153f4-2dcf-4699-83fc-2a08663f9b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_f1 = compute_macro_f1(results, 'zero_shot')\n",
    "few_shot_f1 = compute_macro_f1(results, 'few_shot')\n",
    "\n",
    "print(\"Zero Shot F1 Score: {:.3f}\".format(zero_shot_f1))\n",
    "print(\"Few Shot F1 Score: {:.3f}\".format(few_shot_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a1642b-b16a-423f-8681-3c57d16c82c2",
   "metadata": {},
   "source": [
    "# Chat-GPT-3.5 turbo Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860189ea-ef20-4253-9ee0-afdf1751eba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_base=\"http://91.107.239.71:80\" \n",
    "openai.api_key=\"RfX8Hm8IuOBQGBEMpX4C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a43e75-2f83-4034-8ac7-dfc9f0b9d389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatgpt_labelling_headlines(data, use_few_shot):\n",
    "    all_responses = []\n",
    "\n",
    "    for _, row in tqdm(data.iterrows(), total=len(data)):\n",
    "        instance = row['headline'][:512] \n",
    "        if use_few_shot:\n",
    "            prompt = make_prompt(instance, examples=headlines_examples, zero_shot=False)\n",
    "        else:\n",
    "            prompt = make_prompt(instance, examples=headlines_examples, zero_shot=True)\n",
    "        responses = openai.ChatCompletion.create(model=\"gpt-3.5-turbo-0125\",\n",
    "                                         messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                                         max_tokens = 2, \n",
    "                                         n=1)\n",
    "        \n",
    "        response_list = [row['headline'], row['Change Category']]\n",
    "        response_list.extend([responses['choices'][0]['message']['content']])\n",
    "        all_responses.append(response_list)\n",
    "        \n",
    "        result = pd.DataFrame(all_responses)\n",
    "        result.columns = ['headline', 'label', 'gpt_response']\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f17b7af-bf3f-48ab-a127-20b9d053a63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_responses_zero_shot = chatgpt_labelling_headlines(df, use_few_shot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8004a40-298a-4f25-a040-20b9bfbc054a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_responses_few_shot = chatgpt_labelling_headlines(df, use_few_shot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eddb17-8a2e-49f5-8015-39bb36936f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define replacements\n",
    "replacements = {\n",
    "    'positive': ['Positive', 'positive', 'Positive.', '1)', ' positive'],\n",
    "    'negative': ['Negative', 'negative', '2)', 'Negative.', '\\n\\nNegative'],\n",
    "    'neutral': ['Neutral', 'neutral', 'Neutral.', '3)', ' neutral', 'The headline', 'Neutral ', 'This headline', 'neutral ', '- neutral']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffac2245-97a0-4789-aa67-b4823d4534ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, values in replacements.items():\n",
    "    chat['gpt_response'] = chat['gpt_response'].replace(values, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e6bd20-0316-42c3-ab79-9a117e5d2118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix for zero_shot\n",
    "predicted_labels = chat['label']\n",
    "actual_labels = chat['gpt_response']\n",
    "\n",
    "# Get the unique labels\n",
    "unique_labels = sorted(set(actual_labels) | set(predicted_labels))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(actual_labels, predicted_labels, labels=unique_labels)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=unique_labels, yticklabels=unique_labels)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('Actual Labels')\n",
    "plt.title('Confusion Matrix zero shot ChatGPT')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3add185f-2cd3-4fe1-bf87-b50f647326a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(df, column_name):\n",
    "    y_true = df['label']\n",
    "    y_pred = df[column_name]\n",
    "    f1_macro = round(f1_score(y_true, y_pred, average='macro'), 3)\n",
    "    accuracy = round(accuracy_score(y_true, y_pred), 3)\n",
    "    return {'accuracy': accuracy, 'macro_avg_f1': f1_macro}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0895c040-a68b-478a-ba0c-0d2cd4189725",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_gpt = compute_scores(chat, 'gpt_response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0905df8-e3b7-4556-8f7d-c631cbc3d9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be607c0-eee9-420b-8616-f6a37af97ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts  = chat.groupby('gpt_response').size()\n",
    "category_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1203d427-656d-4458-8b8d-e735c0a08060",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts.plot(kind='bar', color=['royalblue', 'midnightblue', 'slategrey'])\n",
    "\n",
    "plt.title('Predicted Categories: zero-shot mode GPT')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Counts')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6362d86f-f400-4214-aced-e83f068c44bf",
   "metadata": {},
   "source": [
    "# WEB scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb0fdbb-2b80-41f9-acf5-c35dc8fbe972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract website name\n",
    "def get_website_name(url):\n",
    "    match = re.search(r\"(?:https?://)?(?:www\\.)?([a-zA-Z0-9-]+)\\.\", url)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return \"Unknown\"  # return Unknown if no match found\n",
    "\n",
    "filtered_df['website_name'] = filtered_df['url'].apply(get_website_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0262940d-1d99-4318-a312-4d247d215c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the number of articles for all unique sources\n",
    "filtered_df['website_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be77081-4ad0-4bfc-8d5c-d80d6d684f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_html(session, url, headers):\n",
    "    async with session.get(url, headers=headers) as response:\n",
    "        return await response.text()\n",
    "\n",
    "async def parse_html(html, attribute):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        content = soup.find('div', class_ = attribute)\n",
    "        text = content.get_text(strip=True)\n",
    "    except Exception as e:\n",
    "        text = None\n",
    "    return text\n",
    "\n",
    "async def parse_asynchronously(urls, attribute):\n",
    "    headers = {\n",
    "        \"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15\"\n",
    "    }\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [fetch_html(session, url, headers) for url in urls]\n",
    "        html_responses = await asyncio.gather(*tasks)\n",
    "\n",
    "    parsed_texts = await asyncio.gather(*(parse_html(html, attribute) for html in html_responses))\n",
    "    return parsed_texts\n",
    "\n",
    "async def main(df, attribute):\n",
    "    \n",
    "    # List to store parsed texts\n",
    "    article_text = []\n",
    "    \n",
    "    # List of urls \n",
    "    urls = df['url'].tolist()\n",
    "    \n",
    "    # Asynchronously fetch and parse HTML content\n",
    "    parsed_texts = await parse_asynchronously(urls, attribute)\n",
    "    \n",
    "    # Append parsed texts to the result list\n",
    "    article_text.extend(parsed_texts)\n",
    "    \n",
    "    return article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc4fe76-9f60-4fca-891c-ef0672535a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select source\n",
    "marketfy = filtered_df[filtered_df['website_name'] == 'marketfy'].copy()\n",
    "\n",
    "# run the event loop\n",
    "marketfy_texts = await main(marketfy, \"user-content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2ed64a-40a8-479f-a02c-b9100ade4608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to df as an article_body column \n",
    "marketfy['article_body'] = marketfy_texts\n",
    "\n",
    "#save the result \n",
    "marketfy.to_csv('marketfy.csv')\n",
    "\n",
    "#check\n",
    "marketfy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6e6d65-752d-49f0-b92c-8220a50c1687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select source \n",
    "foxbusiness = filtered_df[filtered_df['website_name'] == 'foxbusiness'].copy()\n",
    "\n",
    "# run the event loop\n",
    "foxbusiness_texts = await main(foxbusiness, \"article-content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37811add-fd2a-4ecb-8943-e3ba7fd772b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to df as an article_body column \n",
    "foxbusiness['article_body'] = foxbusiness_texts\n",
    "\n",
    "#save the result \n",
    "foxbusiness.to_csv('foxbusiness.csv')\n",
    "\n",
    "#check\n",
    "foxbusiness.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f20805-5728-4158-8ade-d8a0d154aa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ed020d-c023-4a50-8a56-85b07a7e0170",
   "metadata": {},
   "source": [
    "# Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a859bca5-ed64-43ed-87f9-2e929f948421",
   "metadata": {},
   "source": [
    "## Abstractive summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765659ce-3443-45e7-a38f-5e4d060a1f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate summarization pipeline\n",
    "model_name = \"sshleifer/distilbart-cnn-12-6\"\n",
    "summarizer = pipeline(\"summarization\", model=model_name, tokenizer=model_name, \n",
    "                      framework=\"pt\", truncation=True, device = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a6fedc-56df-4018-a190-a6c0d4476352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted summaries \n",
    "\n",
    "def getSummaryAbstr(summarizer, article):\n",
    "    result = summarizer(article, min_length = 60)\n",
    "    summary = result[0][\"summary_text\"]\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf27f7d-40dc-429e-b61f-29f75e4883ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = pd.read_csv('/kaggle/input/article-full/fulltext_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a66380-eaf4-4f64-93fe-50a53277efb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = full_text['article_body'].astype(str).tolist()\n",
    "full_text[\"summary\"] = full_text['article_body'].progress_apply(lambda row: getSummaryAbstr(summarizer, row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef3f361-1dfa-42cb-a6ee-b24fb4109784",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text.to_csv('full_text_abstractive.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a801d7-09cd-4674-9ef8-1692ade6c3c0",
   "metadata": {},
   "source": [
    "## Extractive summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95287949-6cff-49ca-8754-502a51c110f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model --> model is set to \"bert-base-uncased\" \n",
    "summarizer = Summarizer(model=\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484ec73f-06be-4ce5-9afc-7a71055b7d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSummary(summarizer, article):\n",
    "    result = summarizer(article, min_length = 20)\n",
    "    summary = \"\".join(result)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8b4872-e07d-45b7-9a7e-1bb8457765e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fulltext = pd.read_csv('/kaggle/input/fulltext/fulltext_df.csv')\n",
    "fulltext.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9206be0c-f49a-4b0c-af51-ba9b70b57f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "fulltext[\"summary\"] = fulltext[\"article_body\"].progress_apply(\n",
    "    lambda row: getSummary(summarizer, row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d503d807-5d90-45a9-97a8-781f7acd39b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fulltext.to_csv('full_text_extractive.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51001b01-80eb-4709-aa21-048ab9c0bacb",
   "metadata": {},
   "source": [
    "# Abstractive Summarization ELECTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a7585a-5732-486f-8041-99b2f4fffcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_lab = pd.read_csv('abs_lab.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52b8105-8999-4846-a364-3a61fce4c915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to match expected input ('text' and 'labels')\n",
    "abs_lab['text'] = abs_lab['summary']  # Copying headline to a new column named 'text'\n",
    "abs_lab['label'] = abs_lab['Change Category']  # Renaming 'Change Category' for clarity\n",
    "\n",
    "# Label encoding for 'Change Category'\n",
    "le = LabelEncoder()\n",
    "abs_lab['labels'] = le.fit_transform(abs_lab['label'])\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_df, test_df = train_test_split(abs_lab[['text', 'labels']], test_size=0.2, random_state=42)\n",
    "\n",
    "# Keep only the necessary columns in the train and test sets\n",
    "train_df = train_df[['text', 'labels']]\n",
    "test_df = test_df[['text', 'labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bfd888-f522-483d-84d1-3061c9c39242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model arguments\n",
    "model_args = ClassificationArgs(\n",
    "    num_train_epochs=3,\n",
    "    use_multiprocessing=False,\n",
    "    use_multiprocessing_for_evaluation=False,\n",
    "    output_dir='output',\n",
    "    overwrite_output_dir=True\n",
    ")\n",
    "\n",
    "# Initialize a ClassificationModel using RoBERTa\n",
    "model = ClassificationModel(\n",
    "    \"electra\",\n",
    "    \"google/electra-small-discriminator\",\n",
    "    num_labels=len(le.classes_),\n",
    "    args=model_args,\n",
    "    use_cuda=True\n",
    ")\n",
    "\n",
    "# Train the model on the training dataset\n",
    "model.train_model(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7890b5bf-4304-420b-9c3e-6f1c110fb225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test dataset\n",
    "predictions, raw_outputs = model.predict(test_df['text'].tolist())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_df['labels'], predictions)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343bab1c-1d92-46a6-bf31-a9ae834a3bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example purposes, let's say we have the following:\n",
    "actual_labels = test_df['labels']\n",
    "predicted_labels = predictions  # From your model's predictions on the test set\n",
    "\n",
    "# Calculate precision, recall, F1-score, and support for each class\n",
    "report = classification_report(actual_labels, predicted_labels, target_names=le.classes_)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Calculate and display the confusion matrix\n",
    "cm = confusion_matrix(actual_labels, predicted_labels)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "cm_df = pd.DataFrame(cm, index=le.classes_, columns=le.classes_)\n",
    "print(\"Confusion Matrix with Labels:\\n\", cm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a007bb-c97f-4de5-92db-48711f1fef4e",
   "metadata": {},
   "source": [
    "# Extractive Summarization ELECTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5436b509-0a48-4932-b69c-498f13bf6802",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_lab = pd.read_csv('ext_lab.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cccf358-1b6a-4b52-b1cc-2dd578116eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_lab['summary'] = ext_lab['summary'].fillna('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87e5d9a-6298-4210-9563-2d19d4c2b259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to match expected input ('text' and 'labels')\n",
    "ext_lab['text'] = ext_lab['summary']  # Copying headline to a new column named 'text'\n",
    "ext_lab['label'] = ext_lab['Change Category']  # Renaming 'Change Category' for clarity\n",
    "\n",
    "# Label encoding for 'Change Category'\n",
    "le = LabelEncoder()\n",
    "ext_lab['labels'] = le.fit_transform(ext_lab['label'])\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_df, test_df = train_test_split(ext_lab[['text', 'labels']], test_size=0.2, random_state=42)\n",
    "\n",
    "# Keep only the necessary columns in the train and test sets\n",
    "train_df = train_df[['text', 'labels']]\n",
    "test_df = test_df[['text', 'labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a67c48-3727-4067-a98f-631e0099e6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model arguments\n",
    "model_args = ClassificationArgs(\n",
    "    num_train_epochs=3,\n",
    "    use_multiprocessing=False,\n",
    "    use_multiprocessing_for_evaluation=False,\n",
    "    output_dir='output',\n",
    "    overwrite_output_dir=True\n",
    ")\n",
    "\n",
    "# Initialize a ClassificationModel using RoBERTa\n",
    "model = ClassificationModel(\n",
    "    \"electra\",\n",
    "    \"google/electra-small-discriminator\",\n",
    "    num_labels=len(le.classes_),\n",
    "    args=model_args,\n",
    "    use_cuda=True\n",
    ")\n",
    "\n",
    "# Train the model on the training dataset\n",
    "model.train_model(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d365d0d-02ba-4d81-8164-9513e0279df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test dataset\n",
    "predictions, raw_outputs = model.predict(test_df['text'].tolist())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_df['labels'], predictions)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdbb86a-eed4-4758-832e-77058edf0361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example purposes, let's say we have the following:\n",
    "actual_labels = test_df['labels']\n",
    "predicted_labels = predictions  # From your model's predictions on the test set\n",
    "\n",
    "# Calculate precision, recall, F1-score, and support for each class\n",
    "report = classification_report(actual_labels, predicted_labels, target_names=le.classes_)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Calculate and display the confusion matrix\n",
    "cm = confusion_matrix(actual_labels, predicted_labels)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "cm_df = pd.DataFrame(cm, index=le.classes_, columns=le.classes_)\n",
    "print(\"Confusion Matrix with Labels:\\n\", cm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dc383a-65ed-481b-8955-045417dc17b6",
   "metadata": {},
   "source": [
    "## Abstractive summary results analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61025040-b38b-4558-b4e8-8055286e59c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_summary = pd.read_csv('abs_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1982f8a-7b0b-4b29-9cb1-7c4da0446ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define replacements\n",
    "replacements = {\n",
    "    'positive': ['Positive', 'positive', 'Positive.', '1)'],\n",
    "    'negative': ['Negative', 'negative', '2)'],\n",
    "    'neutral': ['Neutral', 'neutral', 'Neutral.', '3)', ' neutral', 'The headline', 'Once again', 'neutral ']}\n",
    "    \n",
    "# perform replacements\n",
    "for key, values in replacements.items():\n",
    "    abs_summary['zero_shot_gpt'] = abs_summary['zero_shot_gpt'].replace(values, key)\n",
    "    abs_summary['few_shot_gpt'] = abs_summary['few_shot_gpt'].replace(values, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c10fbe-105d-48e4-b544-f48b9eed44f3",
   "metadata": {},
   "source": [
    "### Flan T5 results for abstractive summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138d0ac1-25de-4a5b-8ccc-2620cdde941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of different labels between few shot and zero shot of Flan T5 classification\n",
    "different_values = (abs_summary['zero_shot'] != abs_summary['few_shot']).sum()\n",
    "print(\"Number of different values between 'zero_shot' and 'few_shot':\", different_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b815df2-e263-4ffb-8ee1-412d97c7309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column for comparing true label with zero_shot and few_shot predictions of Flan T5 \n",
    "abs_summary['zero_shot_correct'] = (abs_summary['Change Category'] == abs_summary['zero_shot'])\n",
    "abs_summary['few_shot_correct'] = (abs_summary['Change Category'] == abs_summary['few_shot'])\n",
    "\n",
    "# Calculate the accuracy for zero_shot and few_shot\n",
    "zero_shot_accuracy = abs_summary['zero_shot_correct'].mean() * 100\n",
    "few_shot_accuracy = abs_summary['few_shot_correct'].mean() * 100\n",
    "\n",
    "# Print the accuracies\n",
    "print(\"Zero Shot Accuracy: {:.2f}%\".format(zero_shot_accuracy))\n",
    "print(\"Few Shot Accuracy: {:.2f}%\".format(few_shot_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878cc0ca-fa46-43bc-8efd-b97ce644f477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_macro_f1(df, column_name):\n",
    "    y_true = df['Change Category']\n",
    "    y_pred = df[column_name]\n",
    "    f1 = round(f1_score(y_true, y_pred, average='macro'), 3)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b005e86-b5b5-4af7-9bab-59dbe7d751b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_f1 = compute_macro_f1(abs_summary, 'zero_shot')\n",
    "few_shot_f1 = compute_macro_f1(abs_summary, 'few_shot')\n",
    "\n",
    "print(\"Zero Shot F1 Score: {:.3f}\".format(zero_shot_f1))\n",
    "print(\"Few Shot F1 Score: {:.3f}\".format(few_shot_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a075e7a-48d1-4ed8-b9e9-a60cdac51a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix for zero_shot\n",
    "predicted_labels = abs_summary['zero_shot']\n",
    "actual_labels = abs_summary['Change Category']\n",
    "\n",
    "# Get the unique labels\n",
    "unique_labels = sorted(set(actual_labels) | set(predicted_labels))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(actual_labels, predicted_labels, labels=unique_labels)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=unique_labels, yticklabels=unique_labels)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('Actual Labels')\n",
    "plt.title('Confusion Matrix zero shot GPT for extractive summary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26b32e1-935f-4ff5-9da2-190146476167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix for few_shot\n",
    "predicted_labels = abs_summary['few_shot']\n",
    "actual_labels = abs_summary['Change Category']\n",
    "\n",
    "# Get the unique labels\n",
    "unique_labels = sorted(set(actual_labels) | set(predicted_labels))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(actual_labels, predicted_labels, labels=unique_labels)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=unique_labels, yticklabels=unique_labels)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('Actual Labels')\n",
    "plt.title('Confusion Matrix few shot Flan T5')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605dafb8-1caa-43f3-9444-874fda379e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is there connection between time & correct predictions?\n",
    "\n",
    "abs_summary['date'] = pd.to_datetime(abs_summary['date'])\n",
    "\n",
    "# Extract year from date\n",
    "abs_summary['year'] = abs_summary['date'].dt.year\n",
    "\n",
    "# Count the number of correctly predicted labels for each year\n",
    "correct_predictions = abs_summary[abs_summary['Change Category'] == abs_summary['zero_shot']].groupby('year').size().reset_index(name='correct_predictions')\n",
    "\n",
    "# Merge with total counts to get the proportion of correct predictions\n",
    "total_predictions = abs_summary.groupby('year').size().reset_index(name='total_predictions')\n",
    "analysis_df = pd.merge(total_predictions, correct_predictions, on='year', how='left')\n",
    "\n",
    "# Calculate proportion of correct predictions\n",
    "analysis_df['correct_proportion'] = analysis_df['correct_predictions'] / analysis_df['total_predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3414029-f61d-4f81-888f-0c74cfce0501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of correctly predicted labels for each year \n",
    "correct_predictions_zero_shot = abs_summary[abs_summary['Change Category'] == abs_summary['zero_shot']].groupby('year').size().reset_index(name='correct_predictions_zero_shot')\n",
    "correct_predictions_few_shot = abs_summary[abs_summary['Change Category'] == abs_summary['few_shot']].groupby('year').size().reset_index(name='correct_predictions_few_shot')\n",
    "\n",
    "# Merge with total counts to get the proportion of correct predictions for zero_shot\n",
    "total_predictions_zero_shot = abs_summary.groupby('year').size().reset_index(name='total_predictions')\n",
    "analysis_df = pd.merge(total_predictions_zero_shot, correct_predictions_zero_shot, on='year', how='left')\n",
    "\n",
    "# Merge with total counts to get the proportion of correct predictions for few_shot\n",
    "total_predictions_few_shot = abs_summary.groupby('year').size().reset_index(name='total_predictions_few_shot')\n",
    "analysis_df = pd.merge(analysis_df, correct_predictions_few_shot, on='year', how='left')\n",
    "\n",
    "# Calculate proportion of correct predictions for each mode\n",
    "analysis_df['zero_shot_correct_proportion'] = analysis_df['correct_predictions_zero_shot'] / analysis_df['total_predictions']\n",
    "analysis_df['few_shot_correct_proportion'] = analysis_df['correct_predictions_few_shot'] / analysis_df['total_predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01672aff-bcce-446f-8467-4a5c7e6838b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(analysis_df['year'], analysis_df['zero_shot_correct_proportion'], marker='o', linestyle='-', label='Zero Shot Flant5 Large')\n",
    "plt.plot(analysis_df['year'], analysis_df['few_shot_correct_proportion'], marker='o', linestyle='-', label='Few Shot Flant5 Large')\n",
    "plt.title('Proportion of Correct Predictions Over the Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.xticks(analysis_df['year'])\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7543cfec-7523-4839-a429-112f6ad8fee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts  = abs_summary.groupby('zero_shot').size()\n",
    "category_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adcc50c-b680-44f9-829a-ec2663b93944",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts.plot(kind='bar', color=['royalblue', 'midnightblue', 'slategrey'])\n",
    "\n",
    "plt.title('Predicted Categories: zero-shot mode Flan T5')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Counts')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883300a9-b81f-45cf-8adf-93b1b8faa190",
   "metadata": {},
   "source": [
    "### Chat GPT results for abstractive summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf53c7df-4175-459c-a8bf-3854f3b57b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of different labels between few shot and zero shotof GTP classification\n",
    "different_values = (abs_summary['few_shot_gpt'] != abs_summary['zero_shot_gpt']).sum()\n",
    "print(\"Number of different values between 'zero_shot' and 'few_shot':\", different_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31031f36-06cd-44e6-98d0-408a7ab69f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column for comparing true label with zero_shot and few_shot predictions of Flan T5 \n",
    "abs_summary['zero_shot_correct'] = (abs_summary['Change Category'] == abs_summary['zero_shot_gpt'])\n",
    "abs_summary['few_shot_correct'] = (abs_summary['Change Category'] == abs_summary['few_shot_gpt'])\n",
    "\n",
    "# Calculate the accuracy for zero_shot and few_shot\n",
    "zero_shot_accuracy = abs_summary['zero_shot_correct'].mean() * 100\n",
    "few_shot_accuracy = abs_summary['few_shot_correct'].mean() * 100\n",
    "\n",
    "# Print the accuracies\n",
    "print(\"Zero Shot Accuracy: {:.2f}%\".format(zero_shot_accuracy))\n",
    "print(\"Few Shot Accuracy: {:.2f}%\".format(few_shot_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e131e-51e4-45af-a1fe-64b5bbe4d720",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_f1 = compute_macro_f1(abs_summary, 'zero_shot_gpt')\n",
    "few_shot_f1 = compute_macro_f1(abs_summary, 'few_shot_gpt')\n",
    "\n",
    "print(\"Zero Shot F1 Score: {:.3f}\".format(zero_shot_f1))\n",
    "print(\"Few Shot F1 Score: {:.3f}\".format(few_shot_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8702d330-ebbf-41df-9840-040178338eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is there connection between time & correct predictions?\n",
    "\n",
    "abs_summary['date'] = pd.to_datetime(abs_summary['date'])\n",
    "\n",
    "# Extract year from date\n",
    "abs_summary['year'] = abs_summary['date'].dt.year\n",
    "\n",
    "# Count the number of correctly predicted labels for each year\n",
    "correct_predictions = abs_summary[abs_summary['Change Category'] == abs_summary['zero_shot_gpt']].groupby('year').size().reset_index(name='correct_predictions')\n",
    "\n",
    "# Merge with total counts to get the proportion of correct predictions\n",
    "total_predictions = abs_summary.groupby('year').size().reset_index(name='total_predictions')\n",
    "analysis_df = pd.merge(total_predictions, correct_predictions, on='year', how='left')\n",
    "\n",
    "# Calculate proportion of correct predictions\n",
    "analysis_df['correct_proportion'] = analysis_df['correct_predictions'] / analysis_df['total_predictions']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ff6865-156d-4c6b-a967-ee7f051e77c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of correctly predicted labels for each year \n",
    "correct_predictions_zero_shot = abs_summary[abs_summary['Change Category'] == abs_summary['zero_shot_gpt']].groupby('year').size().reset_index(name='correct_predictions_zero_shot')\n",
    "correct_predictions_few_shot = abs_summary[abs_summary['Change Category'] == abs_summary['few_shot_gpt']].groupby('year').size().reset_index(name='correct_predictions_few_shot')\n",
    "\n",
    "# Merge with total counts to get the proportion of correct predictions for zero_shot\n",
    "total_predictions_zero_shot = abs_summary.groupby('year').size().reset_index(name='total_predictions')\n",
    "analysis_df = pd.merge(total_predictions_zero_shot, correct_predictions_zero_shot, on='year', how='left')\n",
    "\n",
    "# Merge with total counts to get the proportion of correct predictions for few_shot\n",
    "total_predictions_few_shot = abs_summary.groupby('year').size().reset_index(name='total_predictions_few_shot')\n",
    "analysis_df = pd.merge(analysis_df, correct_predictions_few_shot, on='year', how='left')\n",
    "\n",
    "# Calculate proportion of correct predictions for each mode\n",
    "analysis_df['zero_shot_correct_proportion'] = analysis_df['correct_predictions_zero_shot'] / analysis_df['total_predictions']\n",
    "analysis_df['few_shot_correct_proportion'] = analysis_df['correct_predictions_few_shot'] / analysis_df['total_predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c112ab-32e3-429d-abcd-d1c718e05621",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(analysis_df['year'], analysis_df['zero_shot_correct_proportion'], marker='o', linestyle='-', label='Zero Shot GPT')\n",
    "plt.plot(analysis_df['year'], analysis_df['few_shot_correct_proportion'], marker='o', linestyle='-', label='Few Shot GPT')\n",
    "plt.title('Proportion of Correct Predictions Over the Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.xticks(analysis_df['year'])\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3681e36a-f3fe-4346-8792-d1e3ad842fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts  = abs_summary.groupby('zero_shot_gpt').size()\n",
    "category_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d4432e-e872-46b5-8886-3ce7275a8dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts.plot(kind='bar', color=['royalblue', 'midnightblue', 'slategrey'])\n",
    "\n",
    "plt.title('Predicted Categories: zero-shot mode GPT')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Counts')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574c2345-cc29-4336-82de-a8bf83c1ab7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix for few_shot\n",
    "predicted_labels = abs_summary['zero_shot_gpt']\n",
    "actual_labels = abs_summary['Change Category']\n",
    "\n",
    "# Get the unique labels\n",
    "unique_labels = sorted(set(actual_labels) | set(predicted_labels))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(actual_labels, predicted_labels, labels=unique_labels)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=unique_labels, yticklabels=unique_labels)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('Actual Labels')\n",
    "plt.title('Confusion Matrix zero shot GPT')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf2d946-71ab-4bf8-932f-15cae5a64f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix for few_shot\n",
    "predicted_labels = abs_summary['few_shot_gpt']\n",
    "actual_labels = abs_summary['Change Category']\n",
    "\n",
    "# Get the unique labels\n",
    "unique_labels = sorted(set(actual_labels) | set(predicted_labels))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(actual_labels, predicted_labels, labels=unique_labels)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=unique_labels, yticklabels=unique_labels)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('Actual Labels')\n",
    "plt.title('Confusion Matrix few shot GPT')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b97d550-5fd7-471c-950e-93956debaddb",
   "metadata": {},
   "source": [
    "## Extractive summary results analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68619d4e-93f0-42ff-9da9-060152238376",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_summary = pd.read_csv('ext_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95366472-7737-452c-adb7-fb60196e11cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define replacements\n",
    "replacements = {\n",
    "    'positive': ['Positive', 'positive', 'Positive.', '1)'],\n",
    "    'negative': ['Negative', 'negative', '2)'],\n",
    "    'neutral': ['Neutral', 'neutral', 'Neutral.', '3)', 'neutral.', ' neutral', 'neut']}\n",
    "    \n",
    "# perform replacements\n",
    "for key, values in replacements.items():\n",
    "    ext_summary['gpt_zero_shot'] = abs_summary['zero_shot_gpt'].replace(values, key)\n",
    "    ext_summary['gpt_few_shot'] = abs_summary['few_shot_gpt'].replace(values, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57a3f82-4e85-4efa-8877-e1ff5b4301c5",
   "metadata": {},
   "source": [
    "### Flan T5 results for extractive summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181bc2f3-d30b-48df-9595-b00c01f21184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of different labels between few shot and zero shot of Flan T5 classification\n",
    "different_values = (ext_summary['zero_shot'] != ext_summary['few_shot']).sum()\n",
    "print(\"Number of different values between 'zero_shot' and 'few_shot':\", different_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269f0eba-1032-45fc-84b9-5820560de153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column for comparing true label with zero_shot and few_shot predictions of Flan T5 \n",
    "ext_summary['zero_shot_correct'] = (ext_summary['Change Category'] == ext_summary['zero_shot'])\n",
    "ext_summary['few_shot_correct'] = (ext_summary['Change Category'] == ext_summary['few_shot'])\n",
    "\n",
    "# Calculate the accuracy for zero_shot and few_shot\n",
    "zero_shot_accuracy = ext_summary['zero_shot_correct'].mean() * 100\n",
    "few_shot_accuracy = ext_summary['few_shot_correct'].mean() * 100\n",
    "\n",
    "# Print the accuracies\n",
    "print(\"Zero Shot Accuracy: {:.2f}%\".format(zero_shot_accuracy))\n",
    "print(\"Few Shot Accuracy: {:.2f}%\".format(few_shot_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76460ca-f112-4ec5-a628-24340d4ca229",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_f1 = compute_macro_f1(ext_summary, 'zero_shot')\n",
    "few_shot_f1 = compute_macro_f1(ext_summary, 'few_shot')\n",
    "\n",
    "print(\"Zero Shot F1 Score: {:.3f}\".format(zero_shot_f1))\n",
    "print(\"Few Shot F1 Score: {:.3f}\".format(few_shot_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e9274d-ce20-4562-ae27-14a8ba8ff366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix for zero_shot\n",
    "predicted_labels = ext_summary['zero_shot']\n",
    "actual_labels = ext_summary['Change Category']\n",
    "\n",
    "# Get the unique labels\n",
    "unique_labels = sorted(set(actual_labels) | set(predicted_labels))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(actual_labels, predicted_labels, labels=unique_labels)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=unique_labels, yticklabels=unique_labels)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('Actual Labels')\n",
    "plt.title('Confusion Matrix zero shot Flan T5 for extractive summary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04879db-5094-4e4c-a7fa-df0a6591960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix for zero_shot\n",
    "predicted_labels = ext_summary['few_shot']\n",
    "actual_labels = ext_summary['Change Category']\n",
    "\n",
    "# Get the unique labels\n",
    "unique_labels = sorted(set(actual_labels) | set(predicted_labels))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(actual_labels, predicted_labels, labels=unique_labels)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=unique_labels, yticklabels=unique_labels)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('Actual Labels')\n",
    "plt.title('Confusion Matrix few shot Flan T5 for extractive summary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c16a477-da99-406f-b4ce-69ce2776779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is there connection between time & correct predictions?\n",
    "\n",
    "ext_summary['date'] = pd.to_datetime(ext_summary['date'])\n",
    "\n",
    "# Extract year from date\n",
    "ext_summary['year'] = ext_summary['date'].dt.year\n",
    "\n",
    "# Count the number of correctly predicted labels for each year\n",
    "correct_predictions = ext_summary[ext_summary['Change Category'] == ext_summary['zero_shot']].groupby('year').size().reset_index(name='correct_predictions')\n",
    "\n",
    "# Merge with total counts to get the proportion of correct predictions\n",
    "total_predictions = ext_summary.groupby('year').size().reset_index(name='total_predictions')\n",
    "analysis_df = pd.merge(total_predictions, correct_predictions, on='year', how='left')\n",
    "\n",
    "# Calculate proportion of correct predictions\n",
    "analysis_df['correct_proportion'] = analysis_df['correct_predictions'] / analysis_df['total_predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7a22f7-7e9b-4507-a2f1-8c0b395addf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of correctly predicted labels for each year \n",
    "correct_predictions_zero_shot = ext_summary[ext_summary['Change Category'] == ext_summary['zero_shot']].groupby('year').size().reset_index(name='correct_predictions_zero_shot')\n",
    "correct_predictions_few_shot = ext_summary[ext_summary['Change Category'] == ext_summary['few_shot']].groupby('year').size().reset_index(name='correct_predictions_few_shot')\n",
    "\n",
    "# Merge with total counts to get the proportion of correct predictions for zero_shot\n",
    "total_predictions_zero_shot = ext_summary.groupby('year').size().reset_index(name='total_predictions')\n",
    "analysis_df = pd.merge(total_predictions_zero_shot, correct_predictions_zero_shot, on='year', how='left')\n",
    "\n",
    "# Merge with total counts to get the proportion of correct predictions for few_shot\n",
    "total_predictions_few_shot = ext_summary.groupby('year').size().reset_index(name='total_predictions_few_shot')\n",
    "analysis_df = pd.merge(analysis_df, correct_predictions_few_shot, on='year', how='left')\n",
    "\n",
    "# Calculate proportion of correct predictions for each mode\n",
    "analysis_df['zero_shot_correct_proportion'] = analysis_df['correct_predictions_zero_shot'] / analysis_df['total_predictions']\n",
    "analysis_df['few_shot_correct_proportion'] = analysis_df['correct_predictions_few_shot'] / analysis_df['total_predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354f5642-9916-4131-bba2-f2f7555eb442",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(analysis_df['year'], analysis_df['zero_shot_correct_proportion'], marker='o', linestyle='-', label='Zero Shot Flant5 Large for extractive summary')\n",
    "plt.plot(analysis_df['year'], analysis_df['few_shot_correct_proportion'], marker='o', linestyle='-', label='Few Shot Flant5 Large for extractive summary')\n",
    "plt.title('Proportion of Correct Predictions Over the Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.xticks(analysis_df['year'])\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3724f021-4cb2-4171-a763-556d5a286967",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts  = ext_summary.groupby('zero_shot').size()\n",
    "category_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebec2c2-be6e-4467-ab58-1e9cd5bef505",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts.plot(kind='bar', color=['royalblue', 'midnightblue', 'slategrey'])\n",
    "\n",
    "plt.title('Predicted Categories: zero-shot mode Flan T5 for extractive summary')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Counts')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986f797b-5d91-4fee-bb9d-d6490e8984c5",
   "metadata": {},
   "source": [
    "### Chat GPT results for extractive summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab7e605-ad53-4c39-9b1c-f024632063eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of different labels between few shot and zero shot of Flan T5 classification\n",
    "different_values = (ext_summary['gpt_zero_shot'] != ext_summary['gpt_few_shot']).sum()\n",
    "print(\"Number of different values between 'zero_shot' and 'few_shot':\", different_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8391d09e-786a-4640-9118-2d449381e1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column for comparing true label with zero_shot and few_shot predictions of Flan T5 \n",
    "ext_summary['zero_shot_correct'] = (ext_summary['Change Category'] == ext_summary['gpt_zero_shot'])\n",
    "ext_summary['few_shot_correct'] = (ext_summary['Change Category'] == ext_summary['gpt_few_shot'])\n",
    "\n",
    "# Calculate the accuracy for zero_shot and few_shot\n",
    "zero_shot_accuracy = ext_summary['zero_shot_correct'].mean() * 100\n",
    "few_shot_accuracy = ext_summary['few_shot_correct'].mean() * 100\n",
    "\n",
    "# Print the accuracies\n",
    "print(\"Zero Shot Accuracy: {:.2f}%\".format(zero_shot_accuracy))\n",
    "print(\"Few Shot Accuracy: {:.2f}%\".format(few_shot_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0358f491-64f9-4b19-975a-4b9b9f2af31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_f1 = compute_macro_f1(ext_summary, 'gpt_zero_shot')\n",
    "few_shot_f1 = compute_macro_f1(ext_summary, 'gpt_few_shot')\n",
    "\n",
    "print(\"Zero Shot F1 Score: {:.3f}\".format(zero_shot_f1))\n",
    "print(\"Few Shot F1 Score: {:.3f}\".format(few_shot_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b695b8d0-ac91-4868-898d-60ddcdd32545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix for zero_shot\n",
    "predicted_labels = ext_summary['gpt_zero_shot']\n",
    "actual_labels = ext_summary['Change Category']\n",
    "\n",
    "# Get the unique labels\n",
    "unique_labels = sorted(set(actual_labels) | set(predicted_labels))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(actual_labels, predicted_labels, labels=unique_labels)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=unique_labels, yticklabels=unique_labels)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('Actual Labels')\n",
    "plt.title('Confusion Matrix zero shot GPT for extractive summary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e395f3d-a3f7-40f3-9117-d2c7c8c0a0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix for zero_shot\n",
    "predicted_labels = ext_summary['gpt_few_shot']\n",
    "actual_labels = ext_summary['Change Category']\n",
    "\n",
    "# Get the unique labels\n",
    "unique_labels = sorted(set(actual_labels) | set(predicted_labels))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(actual_labels, predicted_labels, labels=unique_labels)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=unique_labels, yticklabels=unique_labels)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('Actual Labels')\n",
    "plt.title('Confusion Matrix few shot GPT for extractive summary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c86ec5c-a469-4663-87e4-c5863277016d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is there connection between time & correct predictions?\n",
    "\n",
    "ext_summary['date'] = pd.to_datetime(ext_summary['date'])\n",
    "\n",
    "# Extract year from date\n",
    "ext_summary['year'] = ext_summary['date'].dt.year\n",
    "\n",
    "# Count the number of correctly predicted labels for each year\n",
    "correct_predictions = ext_summary[ext_summary['Change Category'] == ext_summary['gpt_zero_shot']].groupby('year').size().reset_index(name='correct_predictions')\n",
    "\n",
    "# Merge with total counts to get the proportion of correct predictions\n",
    "total_predictions = ext_summary.groupby('year').size().reset_index(name='total_predictions')\n",
    "analysis_df = pd.merge(total_predictions, correct_predictions, on='year', how='left')\n",
    "\n",
    "# Calculate proportion of correct predictions\n",
    "analysis_df['correct_proportion'] = analysis_df['correct_predictions'] / analysis_df['total_predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a2df35-4fef-4616-8c32-f25fd9b07ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of correctly predicted labels for each year \n",
    "correct_predictions_zero_shot = ext_summary[ext_summary['Change Category'] == ext_summary['gpt_zero_shot']].groupby('year').size().reset_index(name='correct_predictions_zero_shot')\n",
    "correct_predictions_few_shot = ext_summary[ext_summary['Change Category'] == ext_summary['gpt_few_shot']].groupby('year').size().reset_index(name='correct_predictions_few_shot')\n",
    "\n",
    "# Merge with total counts to get the proportion of correct predictions for zero_shot\n",
    "total_predictions_zero_shot = ext_summary.groupby('year').size().reset_index(name='total_predictions')\n",
    "analysis_df = pd.merge(total_predictions_zero_shot, correct_predictions_zero_shot, on='year', how='left')\n",
    "\n",
    "# Merge with total counts to get the proportion of correct predictions for few_shot\n",
    "total_predictions_few_shot = ext_summary.groupby('year').size().reset_index(name='total_predictions_few_shot')\n",
    "analysis_df = pd.merge(analysis_df, correct_predictions_few_shot, on='year', how='left')\n",
    "\n",
    "# Calculate proportion of correct predictions for each mode\n",
    "analysis_df['zero_shot_correct_proportion'] = analysis_df['correct_predictions_zero_shot'] / analysis_df['total_predictions']\n",
    "analysis_df['few_shot_correct_proportion'] = analysis_df['correct_predictions_few_shot'] / analysis_df['total_predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af50d295-f5f5-4a1f-9dd0-fdd7bb016c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(analysis_df['year'], analysis_df['zero_shot_correct_proportion'], marker='o', linestyle='-', label='Zero Shot GPT for extractive summary')\n",
    "plt.plot(analysis_df['year'], analysis_df['few_shot_correct_proportion'], marker='o', linestyle='-', label='Few Shot GPT for extractive summary')\n",
    "plt.title('Proportion of Correct Predictions Over the Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.xticks(analysis_df['year'])\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc221eb-e652-4365-9877-158ea5151c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts  = ext_summary.groupby('gpt_zero_shot').size()\n",
    "category_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44cf2c2-96eb-4429-b5df-38374cf4ceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts.plot(kind='bar', color=['royalblue', 'midnightblue', 'slategrey'])\n",
    "\n",
    "plt.title('Predicted Categories: zero-shot mode GPT for extractive summary')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Counts')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
